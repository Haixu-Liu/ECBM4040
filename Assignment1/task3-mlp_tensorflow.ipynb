{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe6yTh55trpQ"
   },
   "source": [
    "# Assignment 1, Task 3: Multilayer Perceptron (MLP)\n",
    "This is the third part of the assignment. You will get to implement MLP using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Vs2WYIFtrpS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from utils.cifar_utils import load_data\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gYnTjputrpV"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "I31uJ6KltrpW",
    "outputId": "1a677958-43ca-422c-ec66-38e8cdff4283",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 3072) (1000, 3072) (10000, 3072) (100, 3072)\n",
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 3072)\n",
      "Test labels shape:  (10000,)\n",
      "Development data shape: (100, 3072)\n",
      "Development data shape (100,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49,000\n",
    "# Validation data: 1000 samples from original train set: 49,000~50,000\n",
    "# Test data: 10000 samples from original test set: 1~10,000\n",
    "# Development data (for gradient check): 100 from the train set: 1~49,000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xa_w8ZHbtrqY"
   },
   "source": [
    "## Part 1: Tensorflow MLP\n",
    "In this part, you will use tensorflow modules to implement a MLP. We provide a demo of a two-layer net, of which style is referred to https://www.tensorflow.org/guide/keras, and https://www.tensorflow.org/guide/eager. \n",
    "\n",
    "You need to implement a multi-layer with 3 layers in a similar style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Two-layer MLP in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "-0xQOzpdtrqZ",
    "outputId": "212defc2-2cd6-406f-affe-9c6c7367e87f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: valid acc = 0.22499999403953552\n",
      "epoch 2: valid acc = 0.3179999887943268\n",
      "epoch 3: valid acc = 0.3190000057220459\n",
      "epoch 4: valid acc = 0.3310000002384186\n",
      "epoch 5: valid acc = 0.3799999952316284\n",
      "epoch 6: valid acc = 0.4189999997615814\n",
      "epoch 7: valid acc = 0.414000004529953\n",
      "epoch 8: valid acc = 0.4180000126361847\n",
      "epoch 9: valid acc = 0.4399999976158142\n",
      "epoch 10: valid acc = 0.4359999895095825\n",
      "test acc = 0.4401000142097473\n"
     ]
    }
   ],
   "source": [
    "## Demo: Two-layer net in tensorflow (eager execution mode)\n",
    "hidden_dim = 100\n",
    "reg_tf = tf.constant(0.01)\n",
    "\n",
    "# define a tf.keras.Model class\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W1 = tf.Variable(1e-2*np.random.rand(3072, hidden_dim).astype('float32'))\n",
    "        self.b1 = tf.Variable(np.zeros((hidden_dim,)).astype('float32'))\n",
    "        self.W2 = tf.Variable(1e-2*np.random.rand(hidden_dim, 10).astype('float32'))\n",
    "        self.b2 = tf.Variable(np.zeros((10,)).astype('float32'))\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        h1 = tf.nn.relu(tf.matmul(inputs, self.W1) + self.b1)\n",
    "        out = tf.matmul(h1, self.W2) + self.b2\n",
    "        return out\n",
    "\n",
    "# Define and calculate loss function (Note that in eager execution, loss must be in a function)\n",
    "def loss(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    out = model(inputs)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= out, labels=tf.one_hot(targets,10))\n",
    "    L2_loss = tf.nn.l2_loss(model.W1) + tf.nn.l2_loss(model.W2)\n",
    "    return tf.reduce_mean(cross_entropy) + reg * L2_loss\n",
    "\n",
    "# calculate gradients for all variables using tf.GradientTape\n",
    "def grad(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, reg=reg)\n",
    "    return tape.gradient(loss_value, [model.W1, model.b1, model.W2, model.b2])\n",
    "\n",
    "# calculate classification accuracy\n",
    "def eval_acc(model, inputs, targets):\n",
    "    correct_prediction = tf.equal(targets, tf.cast(tf.argmax(model(inputs),1), tf.uint8))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "num_train = 49000\n",
    "batch_size = 500\n",
    "num_batch = num_train//batch_size\n",
    "num_epochs = 10\n",
    "model = Model()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for i in range(num_batch):\n",
    "        batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "        x_tf = tf.Variable(batch_xs, dtype = tf.float32)\n",
    "        y_tf = tf.Variable(batch_ys, dtype = tf.uint8)\n",
    "        \n",
    "        grads = grad(model, x_tf, y_tf, reg_tf)\n",
    "        #optimization based on calculated gradients \n",
    "        optimizer.apply_gradients(zip(grads, [model.W1, model.b1, model.W2, model.b2]))\n",
    "\n",
    "    x_tf = tf.Variable(X_val, dtype = tf.float32)\n",
    "    y_tf = tf.Variable(y_val, dtype = tf.uint8)\n",
    "    accuracy = eval_acc(model, x_tf, y_tf)\n",
    "    val_acc = accuracy.numpy()\n",
    "    print('epoch {}: valid acc = {}'.format(e+1, val_acc))\n",
    "\n",
    "x_tf = tf.Variable(X_test, dtype = tf.float32)\n",
    "y_tf = tf.Variable(y_test, dtype = tf.uint8)\n",
    "accuracy = eval_acc(model, x_tf, y_tf)\n",
    "test_acc = accuracy.numpy()\n",
    "print('test acc = {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Deeper Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YmSduBmytrqb"
   },
   "source": [
    "<p style='color:red'><strong>TODO:</strong></p> \n",
    "\n",
    "Create your MLP in tensorflow. Since you are going to create a deeper neural network, it is recommended to use \"list\" to store your network parameters (weights and bias) and consider to use a loop to create your MLP network. Hint: Copy above code and make necessary changes in model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-IVbxrEtrqd"
   },
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: valid acc = 0.3630000054836273\n",
      "epoch 2: valid acc = 0.4180000126361847\n",
      "epoch 3: valid acc = 0.4390000104904175\n",
      "epoch 4: valid acc = 0.47200000286102295\n",
      "epoch 5: valid acc = 0.4729999899864197\n",
      "epoch 6: valid acc = 0.47600001096725464\n",
      "epoch 7: valid acc = 0.47200000286102295\n",
      "epoch 8: valid acc = 0.47200000286102295\n",
      "epoch 9: valid acc = 0.4749999940395355\n",
      "epoch 10: valid acc = 0.47600001096725464\n",
      "epoch 11: valid acc = 0.47699999809265137\n",
      "epoch 12: valid acc = 0.48100000619888306\n",
      "epoch 13: valid acc = 0.4830000102519989\n",
      "epoch 14: valid acc = 0.492000013589859\n",
      "epoch 15: valid acc = 0.5\n",
      "test acc = 0.48910000920295715\n"
     ]
    }
   ],
   "source": [
    "## Three-layer net in tensorflow (eager execution mode)\n",
    "hidden_dims = [200, 200]\n",
    "reg_tf = tf.constant(0.01)\n",
    "\n",
    "# define a tf.keras.Model class\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, hidden_dims = hidden_dims):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_layers = len(hidden_dims) + 1\n",
    "        self.Ws = []\n",
    "        self.bs = []\n",
    "        dims = [3072] + hidden_dims\n",
    "        for i in range(len(dims)-1):\n",
    "            self.Ws.append(tf.Variable(1e-2*np.random.rand(dims[i], dims[i+1]).astype('float32')))\n",
    "            self.bs.append(tf.Variable(np.zeros((dims[i+1],)).astype('float32')))\n",
    "\n",
    "        self.Ws.append(tf.Variable(1e-2*np.random.rand(dims[-1], 10).astype('float32')))\n",
    "        self.bs.append(tf.Variable(np.zeros((10,)).astype('float32')))\n",
    "    def call(self, inputs):\n",
    "        Ws, bs = self.Ws, self.bs\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        out = tf.nn.relu(tf.matmul(inputs, Ws[0]) + bs[0])\n",
    "        for i in range(1, len(Ws)):\n",
    "            out = tf.matmul(out, Ws[i]) + bs[i]\n",
    "        return out\n",
    "\n",
    "# Define and calculate loss function (Note that in eager execution, loss must be in a function)\n",
    "def loss(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    out = model(inputs)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= out, labels=tf.one_hot(targets,10))\n",
    "    L2_loss = sum([tf.nn.l2_loss(model.Ws[i]) for i in range(len(model.Ws))])\n",
    "    return tf.reduce_mean(cross_entropy) + reg * L2_loss\n",
    "\n",
    "# calculate gradients for all variables using tf.GradientTape\n",
    "def grad(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, reg=reg)\n",
    "    return tape.gradient(loss_value, model.Ws + model.bs)\n",
    "\n",
    "# calculate classification accuracy\n",
    "def eval_acc(model, inputs, targets):\n",
    "    correct_prediction = tf.equal(targets, tf.cast(tf.argmax(model(inputs),1), tf.uint8))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "num_train = 49000\n",
    "batch_size = 500\n",
    "num_batch = num_train//batch_size\n",
    "num_epochs = 15\n",
    "model = Model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for i in range(num_batch):\n",
    "        batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "        x_tf = tf.Variable(batch_xs, dtype = tf.float32)\n",
    "        y_tf = tf.Variable(batch_ys, dtype = tf.uint8)\n",
    "        \n",
    "        grads = grad(model, x_tf, y_tf, reg_tf)\n",
    "        #optimization based on calculated gradients \n",
    "        optimizer.apply_gradients(zip(grads, model.Ws + model.bs))\n",
    "\n",
    "    x_tf = tf.Variable(X_val, dtype = tf.float32)\n",
    "    y_tf = tf.Variable(y_val, dtype = tf.uint8)\n",
    "    accuracy = eval_acc(model, x_tf, y_tf)\n",
    "    val_acc = accuracy.numpy()\n",
    "    print('epoch {}: valid acc = {}'.format(e+1, val_acc))\n",
    "\n",
    "x_tf = tf.Variable(X_test, dtype = tf.float32)\n",
    "y_tf = tf.Variable(y_test, dtype = tf.uint8)\n",
    "accuracy = eval_acc(model, x_tf, y_tf)\n",
    "test_acc = accuracy.numpy()\n",
    "print('test acc = {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: t-SNE (optional, bonus +10 points)\n",
    "\n",
    "t-SNE is is a machine learning algorithm for nonlinear dimensionality reduction developed by Geoffrey Hinton and Laurens van der Maaten. It is also a good way of visualizing high-dimensional data in 2D. We show its application for CIFAR10. Later it will be re-used in a CNN network. Experimenting with t-SNE can be fun. One thing to try is to visualize the output of each layer of MLP to observe the differences.\n",
    "\n",
    "<p style=\"line-height: 1.2;\">[1] Maaten, Laurens van der, and Geoffrey Hinton. \"Visualizing data using t-SNE.\" Journal of Machine Learning Research 9.Nov (2008): 2579-2605.</p>\n",
    "<p style=\"line-height: 1.2;\">[2] Adaptive learning rate scheme by Jacobs https://www.willamette.edu/~gorr/classes/cs449/Momentum/deltabardelta.html</p>\n",
    "<p style=\"line-height: 1.2;\">[3] http://cs.stanford.edu/people/karpathy/cnnembed/</p>\n",
    "<p style=\"line-height: 1.2;\">[4] How to Use t-SNE Effectively, with examples.\n",
    " https://distill.pub/2016/misread-tsne</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_funcs import train, test\n",
    "from utils.classifiers.mlp import MLP\n",
    "from utils.features.tsne import tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "X_train = X_train.reshape([50000,3,32,32]).transpose((0,2,3,1))\n",
    "X_test = X_test.reshape([10000,3,32,32]).transpose((0,2,3,1))\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49000\n",
    "# Validation data: 1000 samples from original train set: 49000~50000\n",
    "# Test data: 10000 samples from original test set: 1~10000\n",
    "# Development data (for gradient check): 100 from the train set: 1~49000 #TODOTA is this 100 or 1000?\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_dev = X_dev.astype(np.float32) - mean_image.astype(np.float32)\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development labels shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tSNE of original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_select = np.random.choice(10000, 500, replace=False)\n",
    "X = X_test[random_select,:,:,0].reshape(500,1024).astype('float')/255.0\n",
    "tic = time.time()\n",
    "Y = tsne(X, low_dim=2, perplexity=30.0)\n",
    "print(\"it takes {} seconds\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize tSNE of original data\n",
    "labels = y_test[random_select]\n",
    "colors = np.random.rand(10,3)\n",
    "color_labels = [colors[int(i)] for i in labels.tolist()]\n",
    "plt.scatter(Y[:,0], Y[:,1], 20, color_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tSNE of data after two hidden layers\n",
    "\n",
    "Do visualization of the tSNE of data after going through MLP. In the visualization result, you should find that in comparison with the tSNE of original data where all data points mess up with each other, tSNE of data after two-layer networks would be shown as multiple clusters in a 2D panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "model = MLP(input_dim=3072, hidden_dims=[100], num_classes=10, reg=0.1, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 200\n",
    "lr = 1e-3\n",
    "verbose = False\n",
    "train_acc_hist, val_acc_hist = train(model, X_train, y_train, X_val, y_val, \n",
    "                  num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, verbose=verbose)\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Visualize data that is passed through MLP model defined above using tSNE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run tSNE\n",
    "X = X_test[random_select]\n",
    "tic = time.time()\n",
    "\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "# Hint: Pass data through affine and dense layers (model.layers) and then \n",
    "# apply softmax to obtain output of the MLP model.\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "print(\"it takes {} seconds\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize tSNE 2D representation of data after two hidden layers\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "# Hint: See tSNE visualization of original data\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Try tuning the parameters of tSNE, do visualization of the new tSNE of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the parameter, show the results.\n",
    "# run tSNE\n",
    "X = X_test[random_select]\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "print(\"it takes {} seconds\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize tSNE 2D representation of data after two hidden layers\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2-mlp_eager.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
