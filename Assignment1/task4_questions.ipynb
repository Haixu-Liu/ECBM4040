{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxw6ESAzoY3k"
   },
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnUnxK0qoY3m"
   },
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNPGiRcKoY3n"
   },
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noMCODnMoY3o"
   },
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSS7CPaLobf5"
   },
   "source": [
    " Because when we use a softmax classifier for multiclass classification problem, what we generate as an output is a probability distribution, according to the property of softmax function. In other words, if we are predicting a data input with 10 possible labels, the output is a vector of size 10, every item of the vector is a probability describing the chance that the input data belongs to this class. Entropy is closely related to distribution, since we want the predicted distribution: p to be more similar to the real distribution: q, cross entropy is a good way to describe the difference between the two distributions. On the other hand, using cross entropy will converge faster compared to other loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmlBV7XXoY3t"
   },
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X573czICoY3u"
   },
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTBv4YxrqaSO"
   },
   "source": [
    "In binary logistic regression, we use sigmoid as the last layer of the model, while in multiclass task we use softmax. We can use one-vs-all method to do multiclass classification task, we can generate K binary classifier, where K is the number of class labels. For every classifier $f_k$, it predicts whether the data belongs to class k, and for all data points we train them on all these classifiers, therefore we use a binary classification method to do multiclass task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu7zI4dzoY3v"
   },
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-aDifP4oY3w"
   },
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfdLyAMpNWtQ"
   },
   "source": [
    "1. The ReLU function is easy to implement, easy to derive and therefore, faster to converge for a neural model\n",
    "2. It is capable of outputting a true zero value\n",
    "3. It provide nonlinearity for the network\n",
    "3. Most of the time the ReLU function is linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb61H2yooY3w"
   },
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVG-XDueOpLA"
   },
   "source": [
    "The general procedure is as follows:\n",
    "* Shuffle the dataset randomly.\n",
    "* Split the dataset into k groups\n",
    "* For each unique group:\n",
    "    * Take the group as a hold out or test data set\n",
    "    * Take the remaining groups as a training data set\n",
    "    * Fit a model on the training set and evaluate it on the test set\n",
    "    * Retain the evaluation score and discard the model\n",
    "* Summarize the skill of the model using the sample of model evaluation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AObuOE0GoY3x"
   },
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyStCkNFoY3y"
   },
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBEZ-KX1oY3y"
   },
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9U2L8quRRx6"
   },
   "source": [
    "First, I tried to change the dimension of the hidden layer, I increased the dimension because I thought for a 10-class classification problem, maybe a more complex model should suffice; then I tried to tune the learning rate and epoch, at the beginning I decreased it, used smaller learning rate and smaller epoch, the result showed that the accuracy decrease dramatically. Therefore, I realized that the learning rate should be larger, and I changed it to 1e-3 and ran it for 15 epochs,the accuracy has already pass 50%, then I tried 300 and 700 as batch size, it didn't help to improve the performance, my final model starts with validation accuracy=0.326, has hidden_dim=200, num_epoch=15, lr=2e-3, and the eventual accuracy is 0.506 for validation and 0.5199 for test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpdq5nqroY3z"
   },
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-Kz3Z0KoY3z"
   },
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task4-questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
